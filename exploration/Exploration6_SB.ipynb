{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SBShimm/Aiffel/blob/master/exploration/Exploration6_SB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 작사가를 만들어보자!"
      ],
      "metadata": {
        "id": "LtGqXUYMqtIK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lvFAEFQ6eQg",
        "outputId": "441be5a3-7e58-4b85-c68b-326d30841611"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.2\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import glob, re\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zp9gQnhH8quj"
      },
      "source": [
        "## 1. 데이터 불러오기  \n",
        "준비되어 있는 노래 가사들이 적혀있는 Text File을 불러옵니다.  \n",
        "불러온 Text File은 Line 단위로 split하여 raw_corpus에 저장해 줍니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYe0UIaf7nXa",
        "outputId": "4c493590-a8ba-42e3-ff9e-fa85ca407580"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "데이터 크기: 187088\n",
            "Examples:\n",
            " ['Looking for some education', 'Made my way into the night', 'All that bullshit conversation']\n"
          ]
        }
      ],
      "source": [
        "txt_file_path = '/content/drive/MyDrive/Colab/Datasets/lyrics/*.txt'\n",
        "\n",
        "txt_list = glob.glob(txt_file_path)\n",
        "\n",
        "raw_corpus = []\n",
        "\n",
        "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
        "for txt_file in txt_list:\n",
        "    with open(txt_file, \"r\") as f:\n",
        "        raw = f.read().splitlines()\n",
        "        raw_corpus.extend(raw)\n",
        "\n",
        "print(\"데이터 크기:\", len(raw_corpus))\n",
        "print(\"Examples:\\n\", raw_corpus[:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiCdmGw18sKi"
      },
      "source": [
        "## 2. 데이터 전처리\n",
        "가사에는 누구의 part인지 (누가 그 부분을 불렀는지) 표시하는 부분이 있습니다. 예를들면 (Kanye: 같은 것) 그런 문장과 빈 문장을 제외하도록 하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, sentence in enumerate(raw_corpus):\n",
        "    if len(sentence) == 0: continue\n",
        "    if sentence[-1] == \":\": continue \n",
        "\n",
        "    if idx > 9: break \n",
        "        \n",
        "    print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v13PLhANr9u3",
        "outputId": "5a97717f-6304-4467-84ea-ca68b96af613"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking for some education\n",
            "Made my way into the night\n",
            "All that bullshit conversation\n",
            "Baby, can't you read the signs? I won't bore you with the details, baby\n",
            "I don't even wanna waste your time\n",
            "Let's just say that maybe\n",
            "You could help me ease my mind\n",
            "I ain't Mr. Right But if you're looking for fast love\n",
            "If that's love in your eyes\n",
            "It's more than enough\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 모델이 문장을 기준을 가지고 잘 학습하기 위해서 토큰화를 해주도록 하겠습니다. 토큰화를 하게 되면 각 단어에 대한 사전이 생성되고 문장의 요소들을 해당 사전에 대한 index로 매핑하여 사용된다. 라고 보면 될 것같습니다.  \n",
        "일단 토큰화에 대한 전처리를 해줍니다. 단어에 특수문자가 포함되거나 대소문자가 구분되어 다른 단어로 인식되지 않게 하고 문장 앞 뒤에 \\<start>와 \\<end>를 추가합시다."
      ],
      "metadata": {
        "id": "VtA-fj-rsmFr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HkY9_Ii78G1S"
      },
      "outputs": [],
      "source": [
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower().strip()\n",
        "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)\n",
        "    sentence = sentence.strip()\n",
        "    sentence = '<start> ' + sentence + ' <end>'\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xA450JgB8UB-"
      },
      "outputs": [],
      "source": [
        "corpus = []\n",
        "\n",
        "for sentence in raw_corpus:\n",
        "    if len(sentence) == 0: continue\n",
        "    if sentence[-1] == \":\": continue\n",
        "    \n",
        "    preprocessed_sentence = preprocess_sentence(sentence)\n",
        "    corpus.append(preprocessed_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eulvMem-bE8"
      },
      "source": [
        "전처리를 마친 문장을 토큰화 하도록 하겠습니다. 여기서 num_words는 사전에 들어갈 단어의 개수라고 보면 됩니다.  \n",
        "토큰화를 진행할 때 각 문장의 길이가 다 다르기 때문에 길이가 짧은 문장은 비는 부분이 0으로 자동으로 패딩됩니다.  \n",
        "너무 긴 문장이 있으면 학습할 가중치가 늘어날테니 maxlen을 설정해 줍시다.  \n",
        "maxlen을 설정하면 기본값으로는 해당 길이만큼의 단어만 토큰화하고 뒷부분은 잘립니다.  \n",
        "다른 파라미터를 조정하여 앞부분을 자를 수도 있지만, 여기서는 그냥 사용하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojQwXoni9rT9",
        "outputId": "0e05d463-a107-45aa-b8de-e79ea29ed336"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  2 304  28 ...   0   0   0]\n",
            " [  2 221  13 ...   0   0   0]\n",
            " [  2  24  17 ...   0   0   0]\n",
            " ...\n",
            " [  2  36   7 ...   0   0   0]\n",
            " [  2  13 440 ...  10  12   3]\n",
            " [  2  26  17 ...   0   0   0]] <keras_preprocessing.text.Tokenizer object at 0x7f365f4fd710>\n"
          ]
        }
      ],
      "source": [
        "def tokenize(corpus):\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        num_words=12000, \n",
        "        filters=' ',\n",
        "        oov_token=\"<unk>\"\n",
        "    )\n",
        "    \n",
        "    tokenizer.fit_on_texts(corpus)\n",
        "    \n",
        "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
        "    \n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=15)  \n",
        "    \n",
        "    print(tensor,tokenizer)\n",
        "    return tensor, tokenizer\n",
        "\n",
        "tensor, tokenizer = tokenize(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tensor[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAPAMaOEuume",
        "outputId": "24522c4c-382a-4d64-fc4f-1d60e961d3c9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kM9d9GhFEk9W"
      },
      "source": [
        "이제 source와 target data를 분리하겠습니다.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D89GWPS5Ekw1",
        "outputId": "c2089557-8357-4a9c-f5b0-794f3f0ddd6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[   2  304   28   99 4811    3    0    0    0    0    0    0    0    0]\n",
            "[ 304   28   99 4811    3    0    0    0    0    0    0    0    0    0]\n"
          ]
        }
      ],
      "source": [
        "src_input = tensor[:, :-1]  \n",
        "tgt_input = tensor[:, 1:]    \n",
        "\n",
        "print(src_input[0])\n",
        "print(tgt_input[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVDIUYhM_PL2"
      },
      "source": [
        "분리한 소스 데이터와 타겟 데이터를 활용하여 train, test 데이터 셋을 나누어줍니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_dy20Lf9-5p3"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, \n",
        "                                                          tgt_input,\n",
        "                                                          test_size=0.2,\n",
        "                                                          shuffle=True, \n",
        "                                                          random_state=1004)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3hBUCUXGJAw"
      },
      "source": [
        "그리고 train data를 이용하여 dataset 객체를 생성해 줍시다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdFn3sW7FT1r",
        "outputId": "09302839-256b-4efe-c40c-b65dedc35434"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset element_spec=(TensorSpec(shape=(128, 14), dtype=tf.int32, name=None), TensorSpec(shape=(128, 14), dtype=tf.int32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "BUFFER_SIZE = len(enc_train)\n",
        "BATCH_SIZE = 128\n",
        "steps_per_epoch = len(enc_train) // BATCH_SIZE\n",
        "\n",
        "VOCAB_SIZE = tokenizer.num_words + 1   \n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tbKYastGH8c"
      },
      "source": [
        "## 3. 모델 학습하기\n",
        "이제 학습을 할차례입니다.  \n",
        "1개의 Embedding 레이어, 2개의 LSTM 레이어, 1개의 Dense 레이어로 구성되는 모델을 사용하도록 하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "I2K7eCiLGHP0"
      },
      "outputs": [],
      "source": [
        "class TextGenerator(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
        "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "    def call(self, x):\n",
        "        out = self.embedding(x)\n",
        "        out = self.rnn_1(out)\n",
        "        out = self.rnn_2(out)\n",
        "        out = self.linear(out)\n",
        "        \n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBQIlBPnGSXt",
        "outputId": "e196dce9-d41a-49cd-a351-34b925d45ee0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "550/550 [==============================] - 64s 100ms/step - loss: 3.7000 - val_loss: 3.3286\n",
            "Epoch 2/20\n",
            "550/550 [==============================] - 54s 99ms/step - loss: 3.2152 - val_loss: 3.1340\n",
            "Epoch 3/20\n",
            "550/550 [==============================] - 54s 99ms/step - loss: 3.0344 - val_loss: 3.0102\n",
            "Epoch 4/20\n",
            "550/550 [==============================] - 54s 99ms/step - loss: 2.8976 - val_loss: 2.9396\n",
            "Epoch 5/20\n",
            "550/550 [==============================] - 54s 99ms/step - loss: 2.7820 - val_loss: 2.8543\n",
            "Epoch 6/20\n",
            "550/550 [==============================] - 54s 99ms/step - loss: 2.6753 - val_loss: 2.7983\n",
            "Epoch 7/20\n",
            "550/550 [==============================] - 54s 99ms/step - loss: 2.5760 - val_loss: 2.7464\n",
            "Epoch 8/20\n",
            "550/550 [==============================] - 54s 99ms/step - loss: 2.4820 - val_loss: 2.7078\n",
            "Epoch 9/20\n",
            "550/550 [==============================] - 54s 99ms/step - loss: 2.3926 - val_loss: 2.6721\n",
            "Epoch 10/20\n",
            "550/550 [==============================] - 54s 99ms/step - loss: 2.3064 - val_loss: 2.6444\n",
            "Epoch 11/20\n",
            "550/550 [==============================] - 54s 99ms/step - loss: 2.2234 - val_loss: 2.6208\n",
            "Epoch 12/20\n",
            "550/550 [==============================] - 54s 99ms/step - loss: 2.1427 - val_loss: 2.5972\n",
            "Epoch 13/20\n",
            "550/550 [==============================] - 54s 99ms/step - loss: 2.0639 - val_loss: 2.5815\n",
            "Epoch 14/20\n",
            "550/550 [==============================] - 54s 99ms/step - loss: 1.9870 - val_loss: 2.5610\n",
            "Epoch 15/20\n",
            "550/550 [==============================] - 54s 99ms/step - loss: 1.9116 - val_loss: 2.5551\n",
            "Epoch 16/20\n",
            "550/550 [==============================] - 54s 99ms/step - loss: 1.8380 - val_loss: 2.5477\n",
            "Epoch 17/20\n",
            "550/550 [==============================] - 54s 99ms/step - loss: 1.7661 - val_loss: 2.5412\n",
            "Epoch 18/20\n",
            "550/550 [==============================] - 54s 99ms/step - loss: 1.6974 - val_loss: 2.5378\n",
            "Epoch 19/20\n",
            "550/550 [==============================] - 54s 99ms/step - loss: 1.6302 - val_loss: 2.5413\n",
            "Epoch 20/20\n",
            "550/550 [==============================] - 54s 99ms/step - loss: 1.5666 - val_loss: 2.5441\n"
          ]
        }
      ],
      "source": [
        "embedding_size = 256\n",
        "hidden_size = 1024\n",
        "epochs = 20\n",
        "\n",
        "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "model.compile(loss=loss, optimizer=optimizer)\n",
        "\n",
        "history = model.fit(enc_train, \n",
        "          dec_train, \n",
        "          epochs=epochs,\n",
        "          batch_size=256,\n",
        "          validation_data=(enc_val, dec_val),\n",
        "          verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HP9ftFZxYes5"
      },
      "source": [
        "## 4. 하이퍼 파라미터 튜닝\n",
        "2.5 즈음에서 더이상 학습이 안되네요. 하이퍼 파라미터를 튜닝하여 더 좋은 결과를 내보도록 합시다.\n",
        "embedding_size는 각 벡터공간에서 단어의 추상적인 표현을 할 수 있는 크기입니다.   \n",
        "hidden_size는 얼마나 많은 일꾼을 사용할 지 입니다.  \n",
        "목표는 10에폭 안에 val_loss를 2.2 아래로 떨어뜨리는 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1_lYoFpYhJ7",
        "outputId": "6d2e32f3-32de-44c7-ddd9-787c396c992b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1098/1098 [==============================] - 171s 143ms/step - loss: 3.6164 - val_loss: 3.1956\n",
            "Epoch 2/10\n",
            "1098/1098 [==============================] - 156s 142ms/step - loss: 3.0355 - val_loss: 2.9467\n",
            "Epoch 3/10\n",
            "1098/1098 [==============================] - 156s 142ms/step - loss: 2.7414 - val_loss: 2.7821\n",
            "Epoch 4/10\n",
            "1098/1098 [==============================] - 156s 142ms/step - loss: 2.4559 - val_loss: 2.6568\n",
            "Epoch 5/10\n",
            "1098/1098 [==============================] - 156s 142ms/step - loss: 2.1797 - val_loss: 2.5696\n",
            "Epoch 6/10\n",
            "1098/1098 [==============================] - 156s 142ms/step - loss: 1.9247 - val_loss: 2.5079\n",
            "Epoch 7/10\n",
            "1098/1098 [==============================] - 156s 142ms/step - loss: 1.7049 - val_loss: 2.4798\n",
            "Epoch 8/10\n",
            "1098/1098 [==============================] - 156s 142ms/step - loss: 1.5203 - val_loss: 2.4718\n",
            "Epoch 9/10\n",
            "1098/1098 [==============================] - 156s 142ms/step - loss: 1.3724 - val_loss: 2.4834\n",
            "Epoch 10/10\n",
            "1098/1098 [==============================] - 157s 143ms/step - loss: 1.2582 - val_loss: 2.5057\n"
          ]
        }
      ],
      "source": [
        "embedding_size = 20\n",
        "hidden_size = 2048\n",
        "epochs = 10\n",
        "\n",
        "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "model.compile(loss=loss, optimizer=optimizer)\n",
        "\n",
        "history = model.fit(dataset,\n",
        "                    epochs=epochs,\n",
        "                    validation_data=(enc_val, dec_val),\n",
        "                    verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.48로 줄기는 했지만 조금더 좋은 결과를 원합니다.  \n",
        "수많은 시도 결과 토큰화 단계에서 설정했던 maxlen을 20으로 설정하고 첫번째 LSTM Layer에 Dropout을 추가한 것이 결과가 제일 좋았습니다."
      ],
      "metadata": {
        "id": "QPbaJI-WGNQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize2(corpus):\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        num_words=12000, \n",
        "        filters=' ',\n",
        "        oov_token=\"<unk>\"\n",
        "    )\n",
        "    \n",
        "    tokenizer.fit_on_texts(corpus)\n",
        "    \n",
        "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
        "    \n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=20)  \n",
        "    \n",
        "    print(tensor,tokenizer)\n",
        "    return tensor, tokenizer\n",
        "\n",
        "tensor, tokenizer = tokenize2(corpus)\n",
        "\n",
        "src_input = tensor[:, :-1]  \n",
        "tgt_input = tensor[:, 1:]    \n",
        "\n",
        "print(src_input[0])\n",
        "print(tgt_input[0])\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, \n",
        "                                                          tgt_input,\n",
        "                                                          test_size=0.2,\n",
        "                                                          shuffle=True, \n",
        "                                                          random_state=1004)\n",
        "\n",
        "BUFFER_SIZE = len(enc_train)\n",
        "BATCH_SIZE = 100\n",
        "steps_per_epoch = len(enc_train) // BATCH_SIZE\n",
        "\n",
        "VOCAB_SIZE = tokenizer.num_words + 1   \n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubXe9hYbGhDu",
        "outputId": "4c432b1d-ac3d-49f0-b556-64f5e48d52dd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  2 304  28 ...   0   0   0]\n",
            " [  2 221  13 ...   0   0   0]\n",
            " [  2  24  17 ...   0   0   0]\n",
            " ...\n",
            " [  2  36   7 ...   0   0   0]\n",
            " [  2  13 440 ...   0   0   0]\n",
            " [  2  26  17 ...   0   0   0]] <keras_preprocessing.text.Tokenizer object at 0x7f36500b4310>\n",
            "[   2  304   28   99 4811    3    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0]\n",
            "[ 304   28   99 4811    3    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset element_spec=(TensorSpec(shape=(100, 19), dtype=tf.int32, name=None), TensorSpec(shape=(100, 19), dtype=tf.int32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "hapL1jBO4WjZ"
      },
      "outputs": [],
      "source": [
        "class TextGenerator2(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size_1, hidden_size_2):\n",
        "        super(TextGenerator2, self).__init__()\n",
        "        \n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
        "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size_1, return_sequences=True, dropout=0.3)\n",
        "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size_2, return_sequences=True)\n",
        "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "    def call(self, x):\n",
        "        out = self.embedding(x)\n",
        "        out = self.rnn_1(out)\n",
        "        out = self.rnn_2(out)\n",
        "        out = self.linear(out)\n",
        "        \n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rd-NYVyF4rn2",
        "outputId": "163982c4-1f99-44ef-9350-d0d928bcb2ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1405/1405 [==============================] - 240s 163ms/step - loss: 2.7244 - val_loss: 2.4182\n",
            "Epoch 2/10\n",
            "1405/1405 [==============================] - 229s 163ms/step - loss: 2.3090 - val_loss: 2.2326\n",
            "Epoch 3/10\n",
            "1405/1405 [==============================] - 229s 163ms/step - loss: 2.0872 - val_loss: 2.1095\n",
            "Epoch 4/10\n",
            "1405/1405 [==============================] - 229s 163ms/step - loss: 1.8777 - val_loss: 2.0242\n",
            "Epoch 5/10\n",
            "1405/1405 [==============================] - 229s 163ms/step - loss: 1.6831 - val_loss: 1.9617\n",
            "Epoch 6/10\n",
            "1405/1405 [==============================] - 229s 163ms/step - loss: 1.5087 - val_loss: 1.9247\n",
            "Epoch 7/10\n",
            "1405/1405 [==============================] - 229s 163ms/step - loss: 1.3593 - val_loss: 1.9062\n",
            "Epoch 8/10\n",
            "1405/1405 [==============================] - 229s 163ms/step - loss: 1.2363 - val_loss: 1.9034\n",
            "Epoch 9/10\n",
            "1405/1405 [==============================] - 229s 163ms/step - loss: 1.1351 - val_loss: 1.9140\n",
            "Epoch 10/10\n",
            "1405/1405 [==============================] - 229s 163ms/step - loss: 1.0542 - val_loss: 1.9292\n"
          ]
        }
      ],
      "source": [
        "embedding_size = 70\n",
        "hidden_size_1 = 2000\n",
        "hidden_size_2 = 2000\n",
        "epochs = 10\n",
        "\n",
        "model = TextGenerator2(tokenizer.num_words + 1, embedding_size , hidden_size_1, hidden_size_2)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "model.compile(loss=loss, optimizer=optimizer)\n",
        "\n",
        "history = model.fit(dataset,\n",
        "                    epochs=epochs,\n",
        "                    validation_data=(enc_val, dec_val),\n",
        "                    verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "loss가 아주 낮아졌다.  "
      ],
      "metadata": {
        "id": "_E57vClQ0qsI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. 가사 작사해보기\n",
        "이제 이 학습모델로 가사를 지어보겠습니다."
      ],
      "metadata": {
        "id": "Od9asIWxd6Me"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "psxZbfffdy2C"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
        "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
        "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
        "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
        "    end_token = tokenizer.word_index[\"<end>\"]\n",
        "\n",
        "    # 단어 하나씩 예측해 문장을 만듭니다\n",
        "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
        "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
        "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
        "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
        "    while True:\n",
        "        # 1\n",
        "        predict = model(test_tensor) \n",
        "        # 2\n",
        "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
        "        # 3 \n",
        "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
        "        # 4\n",
        "        if predict_word.numpy()[0] == end_token: break\n",
        "        if test_tensor.shape[1] >= max_len: break\n",
        "\n",
        "    generated = \"\"\n",
        "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
        "    for word_index in test_tensor[0].numpy():\n",
        "        generated += tokenizer.index_word[word_index] + \" \"\n",
        "\n",
        "    return generated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "nOxdhttudsVV",
        "outputId": "84f7cabc-4aba-4529-a4bc-b19a159af054"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> i love you <end> '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "아주 간단한 문장이... 나왔다"
      ],
      "metadata": {
        "id": "G1iP0xIseE3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> fresh\", max_len=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "jNz4ytvkePhB",
        "outputId": "34f3c455-034f-4624-eed7-e801d3ade7f6"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> fresh out da dealership crackin up wit cigarsin <end> '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "몇가지로 작사를 해봤는데 대부분 학습 데이터의 가사를 가져오는 것 같다.  \n",
        "검색해보니 이미 있는 노래의 가사인 경우가 많았다."
      ],
      "metadata": {
        "id": "JQ4CUC9qfdMM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.회고\n",
        "1. 한번 돌리는데 너무 오래걸려서 너무 힘들었다 일주일 내내 이거 학습만 한듯..\n",
        "2. 일주일 내내 하면서 2.203까지 봤는데 maxlen을 20으로 늘리니까 1.9까지 내려가서 약간의 허탈함이 느껴졌다.\n",
        "3. 임베딩 사이즈를 늘리니까 에폭이 늘어날수록 val_loss가 늘어났다. 임베딩 사이즈가 추상화의 수를 늘리는 거였으니 학습 데이터에 과적합되면서 그런게 아닐까 싶다."
      ],
      "metadata": {
        "id": "A0KVEtQ7fqGE"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "name": "Exploration6-SB.ipynb",
      "provenance": [],
      "mount_file_id": "1E0IpjdSBztVxYhyvJyx05-usOryIUyIW",
      "authorship_tag": "ABX9TyNkD5dLWMQONUyv571/cdAc",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}